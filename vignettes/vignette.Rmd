---
title: "Introducing 'cwbtools'"
author: "Andreas BlÃ¤tte (andreas.blaette@uni-due.de)"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introducing 'cwbtools'}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

# Using the 'cwbtools' package

The 'cwbtools' package offers functionality to create corpora to be used with the Corpus Workbench (CWB), 
or tools building on the CWB from R. It makes the transition from data formats known from well-known R
packages such as tm, quanteda or tidytext to an indexed corpus as smooth as possible. 

The core tool of the cwbtools package is the 'CorpusData'. It incorporates a standard workflow to import and process data, and to generate an indexed corpus. It is implemented as a R6 class due to the reference semantics of the R6 class system. This means, that copying the whole object can be omitted during different processing steps, which can be a wise idea when data are large.


## The CWB directory structure

The CWB stores indexed corpora in individual data directories for each corpus. Corresponding so-called registry files in a 'registry directory', or 'registry' in short describe the corpora.

For the following examples, we create a temporary directory structure for registry files, and indexed corpora.

```{r}
tmpdir <- tempdir()
if (.Platform$OS.type == "windows") tmpdir <- normalizePath(tmpdir, winslash = "/")
registry_tmp <- file.path(tmpdir, "registry")
data_dir_tmp <- file.path(tmpdir, "data_dir")
if (!file.exists(registry_tmp)){
  dir.create (registry_tmp)
} else {
  file.remove(list.files(registry_tmp, full.names = TRUE))
}
if (!file.exists(data_dir_tmp)) dir.create(data_dir_tmp)
```

Trying to keep installation requirements at minimum, we only rely on cwbtools, and the data.table package. Note that the data.table package is used because of its efficiency to handle large data. In addition to speed, it supports in-place modifications of data. When corpora grow large, it is advisable to omit copying the data in memory if not absolutely necessary.

```{r}
library(cwbtools)
library(data.table)
```


## Scenario 1: From XML to CWB - Indexing the UN General Assembly Corpus

The first example is a small sample corpus of debates in the UN General Assembly. The package includes some XML files (TEI standardization).

```{r}
teidir <- system.file(package = "cwbtools", "xml", "UNGA")
teifiles <- list.files(teidir, full.names = TRUE)
list.files(teidir)
```

We create a CorpusData object that will serve as a processor for these files and storage facility for the corpus data. The `chunktable`, `tokenstream` and `metadata`. When we inspect the new object at the outset, we will see that these fields are not filled initially.

```{r unga_instantiate_cd}
unga_data_dir_tmp <- file.path(data_dir_tmp, "unga")
unga_registry_dir <- file.path(registry_tmp, "unga")
if (!file.exists(unga_data_dir_tmp)) dir.create(unga_data_dir_tmp)
file.remove(list.files(unga_data_dir_tmp, full.names = TRUE))
if (file.exists(unga_registry_dir)) file.remove(unga_registry_dir)
UNGA <- CorpusData$new()
UNGA
```

To turn the XML files into the corpusdata object, we use the method `$import_xml`. To be able to add metadata from the header to the metadata table, it requires a named vector of XPath expressions.

```{r basetable, eval = TRUE}
metadata <- c(
  lp = "//legislativePeriod", session = "//titleStmt/sessionNo",
  date = "//publicationStmt/date", url = "//sourceDesc/url",
  src = "//sourceDesc/filetype"
)
UNGA$import_xml(filenames = teifiles, meta = metadata)
UNGA
```

The input XML files are TEI files. Speaker information is included in the attributes of a tag 'sp'. To maintain the original content, there is a element 'speaker' in the document that includes the information on the speaker call without having parsed it. It is not text spoken in the debate, so we need to consider it as noise to be removed.

```{r cleaning, eval = TRUE}
to_keep <- which(is.na(UNGA$metadata[["speaker"]]))
UNGA$chunktable <- UNGA$chunktable[to_keep]
UNGA$metadata <- UNGA$metadata[to_keep][, speaker := NULL]
```

Let us assign nicer column names ... 

```{r}
setnames(UNGA$metadata, old = c("sp_who", "sp_state", "sp_role"), new = c("who", "state", "role"))
```

The CWB requires a tokenstream as input to generate positional attributes. Standard NLP tools will offer lemmatization, part-of-speech-recognition and more at this stage. To keep things simple, we perform a very simple tokenization using the tokenizers pacakge.

```{r dissect, eval = TRUE}
UNGA$tokenize()
UNGA
```

```{r}
UNGA$tokenstream
```

The corpusdata object now includes a table with the token stream. We are ready to import the corpus into the CWB. We use the corpusdata_encode function for this purpose. It is a wrapper on the `p_attribute_encode` function (to encode the token stream), and the `s_attribute_encode` function (to encode the structural attributes / the metadata).

```{r}
s_attrs <- c("id", "who", "state", "role", "lp", "session", "date")
UNGA$encode(
  registry_dir = registry_tmp, data_dir = unga_data_dir_tmp,
  corpus = "UNGA", encoding = "utf8", method = "R",
  p_attributes = "word", s_attributes = character(),
  compress = TRUE
  )
```

Note that we set the logical parameter compress as TRUE. This performs a so-called huffcode compression, which will reduce corpus size and speed up queries. For big corpora, the compression step can be time-consuming. If you want to create a corpus experimentally and do not yet need optimization, performing the compression can be delayed. The indexed corpus has now been prepared. 

The corpus is now ready to be used with CQP (or CQPweb), or a more advanced and convenient environment for corpus analysis, such as 'polmineR'. To gain a quick insight whether everything has worked out as intended (and to keep installation requirements at a minimum), we use the low-level functionality of the RcppCWB package. 

```{r check_use, eval = TRUE}
library(RcppCWB)
id_peace <- cl_str2id(corpus = "UNGA", p_attribute = "word", str = "peace", registry = registry_tmp)
cpos_peace <- cl_id2cpos(corpus = "UNGA", p_attribute = "word", id = id_peace, registry = registry_tmp)

tab <- data.frame(
  i = unlist(lapply(1:length(cpos_peace), function(x) rep(x, times = 11))),
  cpos = unlist(lapply(cpos_peace, function(x) (x - 5):(x + 5)))
  )
tab[["id"]] <- cl_cpos2id(corpus = "UNGA", p_attribute = "word", cpos = tab[["cpos"]], registry = registry_tmp)
tab[["str"]] <- cl_id2str(corpus = "UNGA", p_attribute = "word", id = tab[["id"]], registry = registry_tmp)

peace_context <- split(tab[["str"]], as.factor(tab[["i"]]))
peace_context <- unname(sapply(peace_context, function(x) paste(x, collapse = " ")))
head(peace_context)
```


### Scenario 2: Tidytext to CWB 

In a second scenario, we create a corpus of Jane Austen's books (included in the package janeaustenr). 

```{r}
Austen <- CorpusData$new()
austen_data_dir_tmp <- file.path(data_dir_tmp, "austen")
if (!file.exists(austen_data_dir_tmp)) dir.create(austen_data_dir_tmp)
file.remove(list.files(austen_data_dir_tmp, full.names = TRUE))
```


```{r}
books <- janeaustenr::austen_books()
tbl <- tidytext::unnest_tokens(books, word, text, to_lower = FALSE)
Austen$tokenstream <- as.data.table(tbl)
```

```{r}
Austen$tokenstream[, stem := SnowballC::wordStem(tbl[["word"]], language = "english")]
```

```{r}
Austen$tokenstream[, cpos := 0L:(nrow(tbl) - 1L)]
```

```{r}
cpos_max_min <- function(x) list(cpos_left = min(x[["cpos"]]), cpos_right = max(x[["cpos"]]))
Austen$metadata <- Austen$tokenstream[, cpos_max_min(.SD), by = book]
Austen$metadata[, book := as.character(book)]
setcolorder(Austen$metadata, c("cpos_left", "cpos_right", "book"))
```


```{r}
Austen$tokenstream[, book := NULL]
setcolorder(Austen$tokenstream, c("cpos", "word", "stem"))
Austen$tokenstream
```


```{r}
Austen$encode(
   corpus = "AUSTEN", encoding = "utf8",
   p_attributes = c("word", "stem"), s_attributes = "book",
   registry_dir = registry_tmp, data_dir = austen_data_dir_tmp,
   method = "R", compress = TRUE
)
```

```{r}
corpus <- "AUSTEN"
token <- "pride"
p_attr <- "stem"
id <- cl_str2id(corpus = corpus, p_attribute = p_attr, str = token, registry = registry_tmp)
cpos <- cl_id2cpos(corpus = corpus, p_attribute = p_attr, id = id, registry = registry_tmp)
count <- length(cpos)
count
```


### Scenairo 3: From tm-package VCorpus to CWB

Third, we will make the transition from a VCorpus (tm package) to a CWB indexed corpus. We use the Reuters corpus that is included as sample data in the tm package.

```{r}
library(tm)
reut21578 <- system.file("texts", "crude", package = "tm")
reuters.tm <- VCorpus(DirSource(reut21578), list(reader = readReut21578XMLasPlain))
class(reuters.tm)
```

The easiest way to proceed to the data format we will require for the CorpusData class is to coerce the VCorpus to a tibble, using respective functionality of the tidytext package.

```{r}
library(tidytext)
reuters.tbl <- tidy(reuters.tm)
reuters.tbl
```

Columns we want to encode are still character vectors. We turn the columns with the topic categorizations and the places into a proper character vector.

```{r}
reuters.tbl[["topics_cat"]] <- sapply(
  reuters.tbl[["topics_cat"]],
  function(x) paste(x, collapse = "|")
  )
reuters.tbl[["places"]] <- sapply(
  reuters.tbl[["places"]],
  function(x) paste(x, collapse = "|")
  )
```

This is already the input we need. We instantiate a CorpusData object end make sure that a new data directory for the REUTERS corpus exists (and is empty) ...

```{r}
Reuters <- CorpusData$new()
reuters_data_dir_tmp <- file.path(data_dir_tmp, "reuters")
if (!file.exists(reuters_data_dir_tmp)) dir.create(reuters_data_dir_tmp)
file.remove(list.files(reuters_data_dir_tmp, full.names = TRUE))
```

We assign the chunktable and the metadata, coerced to data.tables, to the object ... 

```{r}
Reuters$chunktable <- data.table(reuters.tbl[, c("id", "text")])
Reuters$metadata <- data.table(reuters.tbl[,c("id", "topics_cat", "places")])
Reuters
```

... we tokenize the text to gain the token stream table ... 

```{r}
Reuters$tokenize()
Reuters
```

... and we are ready to encode the corpus.

```{r}
Reuters$encode(
   corpus = "REUTERS", encoding = "utf8",
   p_attributes = "word", s_attributes = c("topics_cat", "places"),
   registry_dir = registry_tmp,
   data_dir = data_dir_tmp,
   method = "R", compress = TRUE
)
```

The Reuters corpus is about oil production in the Middle East. To check quickly, whether it works, we count the number of occurrences of the word 'oil.

```{r}
id <- cl_str2id(corpus = "REUTERS", p_attribute = "word", str = "oil", registry = registry_tmp)
cpos <- cl_id2cpos(corpus = "REUTERS", p_attribute = "word", id = id, registry = registry_tmp)
count <- length(cpos)
count
```


Hope everything works. Enjoy!